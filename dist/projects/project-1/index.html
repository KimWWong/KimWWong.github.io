<!DOCTYPE html><html lang="en" data-astro-transition-scope="astro-smooz4hq-1"> <head><meta charset="utf-8"><meta name="description" content="Kim Wong's cognitive science research | Kimberly Wong | PhD student at Yale"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="preload" href="https://fonts.googleapis.com/css2?family=Josefin+Sans&family=Pacifico&display=swap" as="style"><link href="https://fonts.googleapis.com/css2?family=Josefin+Sans&family=Pacifico&display=swap" rel="stylesheet"><meta name="generator" content="Astro v4.4.12"><meta name="google-site-verification" content="ZPKGn5dBgJbv6OBn0sSo2xIulB-jTToGn7Wap3zBmek"><title>Seeing Intuitive Physics</title><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><style>.astro-route-announcer{position:absolute;left:0;top:0;clip:rect(0 0 0 0);clip-path:inset(50%);overflow:hidden;white-space:nowrap;width:1px;height:1px}#themeToggle[data-astro-cid-oemx5le4]{border:0;background:none}#themeToggle[data-astro-cid-oemx5le4]:hover{cursor:pointer;rotate:10deg}.moon[data-astro-cid-oemx5le4]{fill:#fdebf3}.sun[data-astro-cid-oemx5le4],.light .moon[data-astro-cid-oemx5le4]{fill:transparent}.light .sun[data-astro-cid-oemx5le4]{fill:#1e1e2e}@keyframes astroFadeInOut{0%{opacity:1}to{opacity:0}}@keyframes astroFadeIn{0%{opacity:0}}@keyframes astroFadeOut{to{opacity:0}}@keyframes astroSlideFromRight{0%{transform:translate(100%)}}@keyframes astroSlideFromLeft{0%{transform:translate(-100%)}}@keyframes astroSlideToRight{to{transform:translate(100%)}}@keyframes astroSlideToLeft{to{transform:translate(-100%)}}@media (prefers-reduced-motion){::view-transition-group(*),::view-transition-old(*),::view-transition-new(*){animation:none!important}[data-astro-transition-scope]{animation:none!important}}
</style>
<link rel="stylesheet" href="/_astro/_slug_.Bs9OKpsp.css" /><script type="module" src="/_astro/hoisted.CjpB8KOH.js"></script><style>[data-astro-transition-scope="astro-smooz4hq-1"] { view-transition-name: astro-smooz4hq-1; }@layer astro { ::view-transition-old(astro-smooz4hq-1) { animation: none; opacity: 0; mix-blend-mode: normal; }::view-transition-new(astro-smooz4hq-1) { animation: none; mix-blend-mode: normal; }::view-transition-group(astro-smooz4hq-1) { animation: none } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-smooz4hq-1"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-smooz4hq-1"] { animation: none; mix-blend-mode: normal; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-smooz4hq-1"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-smooz4hq-1"] { animation: none; mix-blend-mode: normal; }</style><style>[data-astro-transition-scope="astro-xozydmwv-2"] { view-transition-name: astro-xozydmwv-2; }@layer astro { ::view-transition-old(astro-xozydmwv-2) { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }::view-transition-new(astro-xozydmwv-2) { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back]::view-transition-old(astro-xozydmwv-2) { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back]::view-transition-new(astro-xozydmwv-2) { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-xozydmwv-2"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-xozydmwv-2"] { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-xozydmwv-2"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-xozydmwv-2"] { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back][data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-xozydmwv-2"],
			[data-astro-transition=back][data-astro-transition-fallback="old"][data-astro-transition-scope="astro-xozydmwv-2"] { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back][data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-xozydmwv-2"],
			[data-astro-transition=back][data-astro-transition-fallback="new"][data-astro-transition-scope="astro-xozydmwv-2"] { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }</style></head> <body> <header> <nav data-astro-transition-persist="astro-c3mo5b3h-3"> <div class="navbar"> <div class="navbar__title"> <a href="/"><b>Kim Wong</b></a> </div> <div class="navbar__menu"> <a href="/publications/">Publications</a> <a href="/works/">Research</a> <a href="/Kimberly_Wong_CV8.pdf" target="_blank">CV</a> <!--<a href="/cv/">CV</a>--> <!--<a href="https://github.com/ttomczak3/Milky-Way" target="_blank">GitHub</a>--> <button id="themeToggle" title="Theme Toggle" data-astro-cid-oemx5le4> <svg width="20px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" data-astro-cid-oemx5le4> <path class="sun" fill-rule="evenodd" d="M12 17.5a5.5 5.5 0 1 0 0-11 5.5 5.5 0 0 0 0 11zm0 1.5a7 7 0 1 0 0-14 7 7 0 0 0 0 14zm12-7a.8.8 0 0 1-.8.8h-2.4a.8.8 0 0 1 0-1.6h2.4a.8.8 0 0 1 .8.8zM4 12a.8.8 0 0 1-.8.8H.8a.8.8 0 0 1 0-1.6h2.5a.8.8 0 0 1 .8.8zm16.5-8.5a.8.8 0 0 1 0 1l-1.8 1.8a.8.8 0 0 1-1-1l1.7-1.8a.8.8 0 0 1 1 0zM6.3 17.7a.8.8 0 0 1 0 1l-1.7 1.8a.8.8 0 1 1-1-1l1.7-1.8a.8.8 0 0 1 1 0zM12 0a.8.8 0 0 1 .8.8v2.5a.8.8 0 0 1-1.6 0V.8A.8.8 0 0 1 12 0zm0 20a.8.8 0 0 1 .8.8v2.4a.8.8 0 0 1-1.6 0v-2.4a.8.8 0 0 1 .8-.8zM3.5 3.5a.8.8 0 0 1 1 0l1.8 1.8a.8.8 0 1 1-1 1L3.5 4.6a.8.8 0 0 1 0-1zm14.2 14.2a.8.8 0 0 1 1 0l1.8 1.7a.8.8 0 0 1-1 1l-1.8-1.7a.8.8 0 0 1 0-1z" data-astro-cid-oemx5le4></path> <path class="moon" fill-rule="evenodd" d="M16.5 6A10.5 10.5 0 0 1 4.7 16.4 8.5 8.5 0 1 0 16.4 4.7l.1 1.3zm-1.7-2a9 9 0 0 1 .2 2 9 9 0 0 1-11 8.8 9.4 9.4 0 0 1-.8-.3c-.4 0-.8.3-.7.7a10 10 0 0 0 .3.8 10 10 0 0 0 9.2 6 10 10 0 0 0 4-19.2 9.7 9.7 0 0 0-.9-.3c-.3-.1-.7.3-.6.7a9 9 0 0 1 .3.8z" data-astro-cid-oemx5le4></path> </svg> </button>  <script>
    const theme = (() => {
        if (typeof localStorage !== 'undefined' && localStorage.getItem('theme')) {
            return localStorage.getItem('theme');
        }
        // if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        //     return 'dark';
        // }
        return 'light';
    })();

    if (theme === 'dark') {
        document.documentElement.classList.remove('light');
    } else {
        document.documentElement.classList.add('light');
    }

    window.localStorage.setItem('theme', theme);

    const handleToggleClick = () => {
        const element = document.documentElement;
        element.classList.toggle("light");

        const isLight = element.classList.contains("light");
        localStorage.setItem("theme", isLight ? "light" : "dark");
    }

    document.getElementById("themeToggle").addEventListener("click", handleToggleClick);
</script> </div> </div> </nav> </header> <main data-astro-transition-scope="astro-xozydmwv-2">     <h2>Seeing Intuitive Physics</h2>  <p class="p2">The study of 'intuitive physics' really took off just from asking people questions just like these ones below:</p>
    <div style="text-align:center;">
        <img style="max-width: 100%;" class="figure" alt="figure" src="/physics-reasoning.png">
    </div>
<p class="p2">Classic work has demonstrated that most people <b>fail to answer correctly</b>. (If you didn't already know the answer, I've actually given you the correct answers in the image — the dotted arrows are all <i>incorrect</i> trajectories.) </p>
<p class="p2">And such effects generalize to real-world behavior: When asked to drop a ball (while walking) to hit a marked target on the floor, for example, many people release the ball when it is directly over the target, mistakenly predicting that it will fall straight down (McCloskey, 1983). There are two salient themes that emerge from this work:</p>
<p class="p2" style="text-indent: 1em"><b>(1)</b> People are often poor at reasoning about physics</p>
<p class="p2" style="padding-left: 1em"><b>(2)</b> Intuitive physics is centrally a matter of higher-level reasoning and decision-making
</p><p class="p2">And although some recent work in the computational modeling world is beginning to work away at theme 1 (finding the mind's weaknesses/strengths), <i>theme 2 remains unaddressed</i>.
</p><p class="p2"><b><u>The Current Projects</u></b></p>
<p class="p2">The work here illustrates how the two themes provide an incomplete picture of how physical regularities are incorporated into the human mind, and suggests that although people may often be <i>poor at reasoning</i> about physics, their visual percepts themselves reveal a surprising facility with physical principles. In short, we may often be poor at <b><i>thinking</i></b> about physics, but we may nevertheless also be better at <b><i>seeing</i></b> physics.
</p><p class="p2">We are able to disentangle reasoning from seeing for 2 reasons: (1) instead of asking participants direct questions about the physical property in question (e.g. "Is this tower unstable enough to fall down?"), we employed subtler performance-based measures, which observers cannot intentionally control, and (2) the physics of the scene were completely task-irrelevant (and in some cases, participants would have performed better if they ignored the physics entire).
</p><p class="p2">Here you will see studies of intuitive physics in not only rigid objects, but also soft materials such as cloth! </p>
<p></p>
<span class="badge badge--item">RELEVANT PAPERS</span>
<p class="p1">
    Wong, K. W., Bi, W., Soltani, A. A., Yildirim, I., &#x26; Scholl, B. J. (2023). Seeing soft materials draped over objects: A case study of intuitive physics in perception, attention, and memory. <b><i>Psychological Science</i></b>, <i>34</i>(1), 111-119.
    <a class="footer__link" href="https://doi.org/10.1177/09567976221109194" target="_blank">[DOI]</a>
</p>
<p></p><span class="badge badge--item">RELEVANT UNDER REVIEW</span>
<p class="p1">
    Bi, W., Shah, A. D., Wong, K. W., Scholl, B. J., &#x26; Yildirim, I. (<i>submitted</i>). Computational models reveal that intuitive physics underlies visual processing of soft objects.
    </p><details><summary>[Click to show abstract]</summary>
        <p class="p1">
        Computational explorations of human cognition have been especially successful when applied to visual perception. Existing models have primarily focused on rigid objects, emphasizing shapepreserving invariance to changes in viewpoint, lighting, object size, and scene context. Yet many objects in our everyday environments, such as cloths, are soft. This poses both quantitatively greater and qualitatively different challenges for models of perception, due to soft objects’ dynamic and high-dimensional internal structure — as in the changing folds and wrinkles of a cloth waving in the wind. Soft object perception is also correspondingly rich, involving novel properties such as stiffness. Here we explore the ability of different kinds of computational models to capture human visual perception of the physical properties of texture-equated cloths (e.g., their degrees of stiffness) that are undergoing different naturalistic transformations (e.g., falling vs. waving in the wind). Across visual matching tasks, both the successes and failures of human performance are well explained by <i>Woven</i> — a novel model that incorporates physics-based simulations to infer probabilistic representations of cloths. In contrast, competing models that are calibrated to equal performance as Woven on objective measures — including Woven ablations and a deep neural network — fail to capture human performance. We also confirm a novel prediction of Woven in additional analysis of our data. We suggest that humanlike machine vision may also require representations that transcend image features, and involve intuitive physics.
        </p>
    </details>
<p></p><span class="badge badge--item">RELEVANT PRESENTATIONS</span>
<p class="p1">
    Wong, K. W., Shah, A. D., &#x26; Scholl, B. J. (2024). 
    Unconscious intuitive physics: Prioritized breakthrough into visual awareness for physically unstable block towers.
    Talk to be given at the annual meeting of the <b><i>Vision Sciences Society</i></b>,
    5/18/24, St. Pete Beach, FL.
</p>
    <details><summary>[Click to show abstract]</summary>
        <p>
        A central goal of perception and cognition is to predict how events in our local environments are likely to unfold: what is about to happen? And of course some of the most reliable ways of answering this question involve considering the regularities of physics. Accordingly, a great deal of recent research throughout cognitive science has explored the nature of ‘intuitive physics’. The vast majority of this work, however, has involved higher-level reasoning, rather than seeing itself—as when people are asked to deliberate about how objects might move, in response to explicit questions (“Will it fall?”). Here, in contrast, we ask whether the apprehension of certain physical properties of scenes might also occur *unconsciously*, during simple passive viewing. Moreover, we ask whether certain physical regularities are not just processed, but also visually *prioritized*—as when a tower is about to fall. Observers viewed block towers—some stable, some unstable—defined in terms of whether they would collapse as a result of external physical forces (such as gravity) alone. We used continuous flash suppression (CFS) to render the towers initially invisible: observers viewed them monocularly through a mirror haploscope, while a dynamic Mondrian mask was presented to their other eye. We then measured how long towers took to break through this interocular suppression, as observers indicated when they became visually aware of anything other than the mask. The results were clear and striking: unstable towers broke into visual awareness faster than stable towers. And this held even while controlling for other visual properties—e.g. while contrasting pairs of stable vs. unstable towers sharing the same convex hull, and differing only in the horizontal placement of a single block. This work shows how physical instability is both detected and prioritized, not only during overt deliberation, but also in unconscious visual processing.
        </p>
    </details>
<p class="p1">
    Shah, A. D., Wong, K. W., Ilker, Y., &#x26; Scholl, B. J. (2023).
    Perceiving precarity (beyond instability) in block towers.
    Poster presented at the annual meeting of the <b><i>Vision Sciences Society</i></b>, 
    5/23/21, Online.  
</p>
    <details><summary>[Click to show abstract]</summary>
        <p>
        Intuitive physics has traditionally been associated with higher-level cognition, but recent work has also focused on the exciting possibility that properties such as physical stability may be rapidly and spontaneously extracted as a part of seeing itself — as when you look at a tower of blocks, and can appreciate at a glance that it is about to topple. Much of this work has contrasted towers that appear stable vs. unstable, in terms of whether they would fall as a result of external physical forces (such as gravity) alone. But the 'perception of physics' in block towers seems richer than a binary stable/unstable state. Even when a tower is (and appears to be) stable, for example, we might still readily perceive how precarious it is — in terms of how much force would be required in order to knock it over. Here we explored perceived 'precariousness' using change detection. Observers viewed pairs of block-tower images (one at a time, separated by a mask), and simply reported whether the second image was different. The towers were always stable, but could be differentially precarious. On More-Precarious trials, a single block was shifted slightly so that the tower became less resistant to falling (as quantified by physics-based simulations with variable amounts of spatial jitter). On corresponding Less-Precarious trials, that same block was shifted slightly so that the tower became more resistant to falling. We expected greater attention to (and memory for) changes that introduced a greater likelihood of collapse. But we obtained exactly the opposite pattern: observers were far better at detecting changes on Less-Precarious trials, compared to More-Precarious trials. We explore the possibility that this surprising result may be explained by the 'perception of history', in terms of appreciating how such towers were constructed in the first place.
        </p>
    </details>
<p class="p1">
    Bi, W., Shah, A. D. Wong, K. W., Scholl, B. J, &#x26; Yildirim, I. (2021).
    Perception of soft materials relies on physics-based object representations: Behavioral and computational evidence.
    Poster presented at the annual meeting of the <b><i>Vision Sciences Society</i></b>, 
    5/23/21, Online.  
</p>
    <details><summary>[Click to show abstract]</summary>
        <p>
        When encountering objects, we readily perceive not only low-level properties (e.g., color and orientation), but also seemingly higher-level ones — some of which seem to involve aspects of physics (e.g., mass). Perhaps nowhere is this contrast more salient than in the perception of soft materials such as cloths: the dynamics of these objects (including how their three-dimensional forms vary) are determined by their physical properties such as stiffness, elasticity, and mass. Here we argue that the perception of cloths and their physical properties must involve not only image statistics, but also abstract object representations that incorporate "intuitive physics". We do so by exploring the ability to <i>generalize</i> across very different image statistics in both visual matching and computational modeling. Behaviorally, observers had to visually match the stiffness of animated cloths reacting to external forces and undergoing natural transformations (e.g. flapping in the wind, or falling onto the floor). Matching performance was robust despite massive variability in the lower-level image statistics (including those due to location and orientation perturbations) and the higher-level variability in both extrinsic scene forces (e.g., wind vs. rigid-body collision) and intrinsic cloth properties (e.g., mass). We then confirmed that this type of generalization can be explained by a computational model in which, given an input animation, cloth perception amounts to inverting a probabilistic physics-based simulation process. Only this model — and neither the alternatives relying exclusively on simpler representations (e.g., dynamic image features such as velocity coherence) nor alternatives based on deep learning approaches — was able to explain observed behavioral patterns. These behavioral and computational results suggest that the perception of soft materials is governed by a form of "intuitive physics" — an abstract, physics-based representation of approximate cloth mechanics that explains observed shape variations in terms of how unobservable properties determine cloth reaction to external forces.
        </p>
    </details>
<p class="p1">
    Wong, K. W., Bi, W., Yildirim, I., &#x26; Scholl, B. J. (2021). 
    Seeing cloth-covered objects: A case study of intuitive physics in perception, attention, and memory. 
    Poster presented at the annual meeting of the <b><i>Vision Sciences Society</i></b>, 
    5/23/21, Online.  
</p>
    <details><summary>[Click to show abstract]</summary>
        <p>
        We typically think of intuitive physics in terms of high-level cognition, but might aspects of physics also be extracted during lower-level visual processing? In short, might we not only *think* about physics, but also *see* it? We explored this in the context of *covered* objects — as when you see a chair with a blanket draped over it. To successfully recover the underlying structure of such scenes (and determine which image components reflect the object itself), we must account for the physical interactions between cloth, gravity, and object — which govern not only the way the cloth may wrinkle and fold on itself, but also the way it hangs across the object's edges and corners. We explored this using change detection: Observers saw two images of cloth-covered objects appear quickly one after the other, and simply had to detect whether the two raw images were identical. On "Same Object" trials, the superficial folds and creases of the cloth changed dramatically, but the underlying object was identical (as might happen if you threw a blanket onto a chair repeatedly). On "Different Object" trials, in contrast, both the cloth and the underlying covered object changed. Critically, "Same Object" trials always had *greater* visual change than "Different Object" trials — in terms of both brute image metrics (e.g. the number of changed pixels) and higher-level features (as quantified by distance in vectorized feature-activation maps from relatively late layers in a convolutional neural network trained for object recognition [VGG16]). Observers were far better at detecting changes on "Different Object" trials, despite the lesser degree of overall visual change. Just as vision "discounts the illuminant" to recover the deeper property of reflectance in lightness perception, visual processing uses intuitive physics to "discount the cloth" in order to recover the deeper underlying structure of objects.
        </p>
    </details>             <footer> <div class="center"> <ul class="footer"> <!--<li class="icon__btn"><a class="icon__link" href="#" target="_blank"><span class="git-icon">GitHub</span></a></li>--> <!--<li class="icon__btn"><a class="icon__link" href="mailto: kimberly.wong@yale.edu"><span class="mail-icon">Email</span></a></li>--> <!--<li class="icon__btn"><a class="icon__link" href="#" target="_blank"><span class="linked-in">LinkedIn</span></a></li>--> </ul> <small>&copy; 2024 Milky-Way theme by <a class="footer__link" href="https://github.com/ttomczak3" target="_blank">ttomczak</a>. All Rights Reserved.</small> </div> </footer> </main>  </body> </html>