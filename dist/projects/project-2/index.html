<!DOCTYPE html><html lang="en" data-astro-transition-scope="astro-smooz4hq-1"> <head><meta charset="utf-8"><meta name="description" content="Kim Wong's cognitive science research | Kimberly Wong | PhD student at Yale"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="preload" href="https://fonts.googleapis.com/css2?family=Josefin+Sans&family=Pacifico&display=swap" as="style"><link href="https://fonts.googleapis.com/css2?family=Josefin+Sans&family=Pacifico&display=swap" rel="stylesheet"><meta name="generator" content="Astro v4.4.12"><meta name="google-site-verification" content="SOeo9eE42oMgYMGjb4E1hRGsqrGgKE2GlaSxa8HR2xA"><title>Spontaneous Visual Routines</title><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><style>.astro-route-announcer{position:absolute;left:0;top:0;clip:rect(0 0 0 0);clip-path:inset(50%);overflow:hidden;white-space:nowrap;width:1px;height:1px}#themeToggle[data-astro-cid-oemx5le4]{border:0;background:none}#themeToggle[data-astro-cid-oemx5le4]:hover{cursor:pointer;rotate:10deg}.moon[data-astro-cid-oemx5le4]{fill:#fdebf3}.sun[data-astro-cid-oemx5le4],.light .moon[data-astro-cid-oemx5le4]{fill:transparent}.light .sun[data-astro-cid-oemx5le4]{fill:#1e1e2e}@keyframes astroFadeInOut{0%{opacity:1}to{opacity:0}}@keyframes astroFadeIn{0%{opacity:0}}@keyframes astroFadeOut{to{opacity:0}}@keyframes astroSlideFromRight{0%{transform:translate(100%)}}@keyframes astroSlideFromLeft{0%{transform:translate(-100%)}}@keyframes astroSlideToRight{to{transform:translate(100%)}}@keyframes astroSlideToLeft{to{transform:translate(-100%)}}@media (prefers-reduced-motion){::view-transition-group(*),::view-transition-old(*),::view-transition-new(*){animation:none!important}[data-astro-transition-scope]{animation:none!important}}
</style>
<link rel="stylesheet" href="/_astro/_slug_.DA1BPMx9.css" /><script type="module" src="/_astro/hoisted.CjpB8KOH.js"></script><style>[data-astro-transition-scope="astro-smooz4hq-1"] { view-transition-name: astro-smooz4hq-1; }@layer astro { ::view-transition-old(astro-smooz4hq-1) { animation: none; opacity: 0; mix-blend-mode: normal; }::view-transition-new(astro-smooz4hq-1) { animation: none; mix-blend-mode: normal; }::view-transition-group(astro-smooz4hq-1) { animation: none } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-smooz4hq-1"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-smooz4hq-1"] { animation: none; mix-blend-mode: normal; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-smooz4hq-1"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-smooz4hq-1"] { animation: none; mix-blend-mode: normal; }</style><style>[data-astro-transition-scope="astro-xozydmwv-2"] { view-transition-name: astro-xozydmwv-2; }@layer astro { ::view-transition-old(astro-xozydmwv-2) { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }::view-transition-new(astro-xozydmwv-2) { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back]::view-transition-old(astro-xozydmwv-2) { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back]::view-transition-new(astro-xozydmwv-2) { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-xozydmwv-2"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-xozydmwv-2"] { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-xozydmwv-2"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-xozydmwv-2"] { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back][data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-xozydmwv-2"],
			[data-astro-transition=back][data-astro-transition-fallback="old"][data-astro-transition-scope="astro-xozydmwv-2"] { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back][data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-xozydmwv-2"],
			[data-astro-transition=back][data-astro-transition-fallback="new"][data-astro-transition-scope="astro-xozydmwv-2"] { 
	animation-duration: 0.2s;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }</style></head> <body> <header> <nav data-astro-transition-persist="astro-c3mo5b3h-3"> <div class="navbar"> <div class="navbar__title"> <a href="/"><b>Kim Wong</b></a> </div> <div class="navbar__menu"> <a href="/publications/">Publications</a> <a href="/works/">Research</a> <a href="/Kimberly_Wong_CV.pdf" target="_blank">CV</a> <!--<a href="/cv/">CV</a>--> <!--<a href="https://github.com/ttomczak3/Milky-Way" target="_blank">GitHub</a>--> <button id="themeToggle" title="Theme Toggle" data-astro-cid-oemx5le4> <svg width="20px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" data-astro-cid-oemx5le4> <path class="sun" fill-rule="evenodd" d="M12 17.5a5.5 5.5 0 1 0 0-11 5.5 5.5 0 0 0 0 11zm0 1.5a7 7 0 1 0 0-14 7 7 0 0 0 0 14zm12-7a.8.8 0 0 1-.8.8h-2.4a.8.8 0 0 1 0-1.6h2.4a.8.8 0 0 1 .8.8zM4 12a.8.8 0 0 1-.8.8H.8a.8.8 0 0 1 0-1.6h2.5a.8.8 0 0 1 .8.8zm16.5-8.5a.8.8 0 0 1 0 1l-1.8 1.8a.8.8 0 0 1-1-1l1.7-1.8a.8.8 0 0 1 1 0zM6.3 17.7a.8.8 0 0 1 0 1l-1.7 1.8a.8.8 0 1 1-1-1l1.7-1.8a.8.8 0 0 1 1 0zM12 0a.8.8 0 0 1 .8.8v2.5a.8.8 0 0 1-1.6 0V.8A.8.8 0 0 1 12 0zm0 20a.8.8 0 0 1 .8.8v2.4a.8.8 0 0 1-1.6 0v-2.4a.8.8 0 0 1 .8-.8zM3.5 3.5a.8.8 0 0 1 1 0l1.8 1.8a.8.8 0 1 1-1 1L3.5 4.6a.8.8 0 0 1 0-1zm14.2 14.2a.8.8 0 0 1 1 0l1.8 1.7a.8.8 0 0 1-1 1l-1.8-1.7a.8.8 0 0 1 0-1z" data-astro-cid-oemx5le4></path> <path class="moon" fill-rule="evenodd" d="M16.5 6A10.5 10.5 0 0 1 4.7 16.4 8.5 8.5 0 1 0 16.4 4.7l.1 1.3zm-1.7-2a9 9 0 0 1 .2 2 9 9 0 0 1-11 8.8 9.4 9.4 0 0 1-.8-.3c-.4 0-.8.3-.7.7a10 10 0 0 0 .3.8 10 10 0 0 0 9.2 6 10 10 0 0 0 4-19.2 9.7 9.7 0 0 0-.9-.3c-.3-.1-.7.3-.6.7a9 9 0 0 1 .3.8z" data-astro-cid-oemx5le4></path> </svg> </button>  <script>
    const theme = (() => {
        if (typeof localStorage !== 'undefined' && localStorage.getItem('theme')) {
            return localStorage.getItem('theme');
        }
        // if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        //     return 'dark';
        // }
        return 'light';
    })();

    if (theme === 'dark') {
        document.documentElement.classList.remove('light');
    } else {
        document.documentElement.classList.add('light');
    }

    window.localStorage.setItem('theme', theme);

    const handleToggleClick = () => {
        const element = document.documentElement;
        element.classList.toggle("light");

        const isLight = element.classList.contains("light");
        localStorage.setItem("theme", isLight ? "light" : "dark");
    }

    document.getElementById("themeToggle").addEventListener("click", handleToggleClick);
</script> </div> </div> </nav> </header> <main data-astro-transition-scope="astro-xozydmwv-2">     <h2>Spontaneous Visual Routines</h2>  <p class="p2"><b><u>What actually is a "Visual Routine"?</u></b></p>
<p class="p2">Visual processing usually seems both incidental and instantaneous.  (You see color, for example, without needing to <i>try</i>, and without any noticeable lag.)  There are, however, fascinating exceptions to this rule.  For example, take a moment to glance at this picture before moving on.</p>
    <div style="text-align:center;">
        <img style="max-width: 80%;" class="figure" alt="figure" src="/shoelaces.png">
    </div>
<p class="p2">Now consider some questions that could be asked about that picture (without looking back at it!)</p>
<p class="p2" style="text-indent: 1em"><b>(1)</b> What colors were in the picture?</p>
<p class="p2">You can probably answer that question immediately, from memory — indicating that this property was extracted even before you were asked the question, just as a part of natural viewing.</p>
<p class="p2">But what about this question: </p>
<p class="p2" style="padding-left: 1em"><b>(2)</b> Were the green tip and the blue tip part of the same shoelace, or two different shoelaces?  </p>
<p class="p2">You probably don’t know the answer yet, which indicates that this property was not extracted incidentally during natural viewing.  When you look back at the shoelace picture, of course, you can answer this question too — and you can do so merely by <i>looking</i> (i.e. even without using your finger to follow along a lace).  But notice that even here you can’t answer the question <i>immediately</i>: whereas you see the laces’ color seemingly instantaneously, seeing which tip goes with which seems to involve a process that is appreciably deliberate, dynamic, and temporally extended (as you mentally ‘trace’ from one tip to another).</p>
<p class="p2">These types of visual operations that underlies your ability to answer the which-tip-goes-with-which question has been termed <b>‘visual routines’</b> (Ullman, 1984, 1996), and visual routines contrast with other forms of perception precisely in terms of the two features highlighted in that example: they are often invoked <b>only on demand</b> (rather than always occurring automatically), and they are inherently, appreciably <b>dynamic</b>, such that these operations often take some appreciable time to be executed.</p>
<p class="p2">Not only has there been discussion of how visual routines may lie at the root of well-known visual processes such as: figure-ground relationships, containment (e.g. “Is it inside?”), and connectedness (e.g. “Are both points on the same contour?”), but they have been prominent enough to be reviewed in introductory textbooks (e.g. Palmer, 1999). </p>
<p class="p2"><b><u>The Current Project</u></b></p>
<p class="p2">The work here studies visual routines under a new lens, re-examining the assumptions that traditionally defined visual routines, and exploring new contexts where visual routines may operate.  (Spoiler: <i>mazes!</i>)</p>
    <div style="margin-left: 0em">
        <img height="200" class="figure" alt="figure" src="/visroutines-mazedemo-trans.png">
    </div>
<p></p><span class="badge badge--item">RELEVANT PUBLICATION</span>
<p class="p1">
    Wong, K. W., &#x26; Scholl, B. J. (2024). Spontaneous path tracing in task-irrelevant mazes: Spatial affordances trigger dynamic visual routines.
    <b><i>Journal of Experimental Psychology: General</i></b>. Advance online publication.
    <br><a class="footer__link" href="https://doi.org/10.1037/xge0001618" target="_blank">[DOI]</a> <a class="footer__link" href="/Wong-Scholl-JEPG.pdf" target="_blank">[PDF]</a>
</p>
<p></p>
<span class="badge badge--item">RELEVANT PRESENTATIONS</span>
<p class="p1">
    Wong, K. W., &#x26; Scholl, B. J. (2023). 
    What memories are formed by dynamic 'visual routines'? 
    Poster presented at the annual meeting of the <b><i>Vision Sciences Society</i></b>, 
    5/22/23, St. Pete Beach, FL.  
</p>
    <details><summary>[Click to show abstract]</summary>
        <p>
        You can readily see at a glance how two objects spatially relate to each other. But seeing how 20 objects all relate seems impossible, due to computational explosion (with 190 pairs). Such situations require <i>visual routines</i>: dynamic visual procedures that efficiently compute various properties 'on demand' -- e.g. whether two points lie on the same winding path, in a busy scene containing many points and paths ('path tracing'). Some surprisingly foundational questions about visual routines remain unexplored, including: what (if anything) remains in visual memory after the execution of a visual routine? Does path tracing result in a memory of the traced path itself? Or just of <i>whether</i> there was a path? Or nothing at all, after the moment has passed? We explored this for spontaneous path tracing in 2D mazes. Observers saw a maze in which two probes appeared in positions connected by a path. They were then shown two mazes, and had to select which was the initially presented maze. Across experiments, the incorrect maze could be (1) a Path-Obstruction maze, where a new contour blocked the initial inter-probe path; (2) an Irrelevant-Obstruction maze, where a new contour was introduced elsewhere; or (3) an Alternative-Path maze, where the same new Path-Obstruction contour was accompanied by the removal of an existing contour, providing an alternative inter-probe path. Performance on Path-Obstruction trials was much better than on Irrelevant-Obstruction trials (always controlling for lower-level contour properties across trial types). But Alternative-Path trials entirely eliminated this advantage. This suggests that a visual memory is formed by spontaneous path tracing, but that its content is not the path itself, but only <i>whether</i> a path existed. If visual routines exist to answer on-demand questions during perception, then the resulting memories may consist only of the answers themselves, and not the processing that generated them.        
        </p>
    </details>
<p class="p1">
    Wong, K. W., &#x26; Scholl, B. J. (2023). 
    Spatial affordances can automatically trigger dynamic visual routines: Spontaneous path tracing in task-irrelevant mazes.
    Talk presented at the annual meeting of the <b><i>Vision Sciences Society</i></b>, 
    5/14/23, St. Pete Beach, FL.  
</p>
    <details><summary>[Click to show abstract]</summary>
        <p>
        Visual processing usually seems both incidental and instantaneous. But imagine viewing a jumble of shoelaces, and wondering whether two particular tips are part of the same lace. You can answer this by looking, but doing so may require something dynamic happening in vision (as the lace is effectively 'traced'). Such tasks are thought to involve 'visual routines': dynamic visual procedures that efficiently compute various properties on demand, such as whether two points lie on the same curve. Past work has suggested that visual routines are invoked by observers' particular (conscious, voluntary) goals, but here we explore the possibility that some visual routines may also be automatically triggered by certain stimuli themselves. In short, we suggest that certain stimuli effectively <i>afford</i> the operation of particular visual routines (as in Gibsonian affordances). We explored this using stimuli that are familiar in everyday experience, yet relatively novel in human vision science: mazes. You might often solve mazes by drawing paths with a pencil -- but even without a pencil, you might find yourself tracing along various paths <i>mentally</i>. Observers had to compare the visual properties of two probes that were presented along the paths of a maze. Critically, the maze itself was entirely task-irrelevant, but we predicted that simply <i>seeing</i> the visual structure of a maze in the first place would afford automatic mental path tracing. Observers were indeed slower to compare probes that were further from each other along the paths, even when controlling for lower-level visual properties (such as the probes' brute linear separation, i.e. ignoring the maze 'walls'). This novel combination of two prominent themes from our field -- affordances and visual routines -- suggests that at least some visual routines may operate in an automatic (fast, incidental, and stimulus-driven) fashion, as a part of basic visual processing itself.
        </p>
    </details>             <footer> <div class="center"> <ul class="footer"> <!--<li class="icon__btn"><a class="icon__link" href="#" target="_blank"><span class="git-icon">GitHub</span></a></li>--> <!--<li class="icon__btn"><a class="icon__link" href="mailto: kimberly.wong@yale.edu"><span class="mail-icon">Email</span></a></li>--> <!--<li class="icon__btn"><a class="icon__link" href="#" target="_blank"><span class="linked-in">LinkedIn</span></a></li>--> </ul> <small>&copy; 2024 Milky-Way theme by <a class="footer__link" href="https://github.com/ttomczak3" target="_blank">ttomczak</a>. All Rights Reserved.</small> </div> </footer> </main>  </body> </html>